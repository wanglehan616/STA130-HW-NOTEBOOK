{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "# Check for missing values per column\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "# Summary of missing values\n",
    "missing_summary = df.isnull().sum().reset_index()\n",
    "missing_summary.columns = ['Column', 'Missing Values']\n",
    "print(missing_summary)\n",
    "# Total number of missing values\n",
    "total_missing = df.isnull().sum().sum()\n",
    "print(f'Total missing values: {total_missing}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Get the number of rows and columns\n",
    "rows, columns = df.shape\n",
    "print(f'The dataset has {rows} rows and {columns} columns.')\n",
    "# Check for missing values per column\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "# Summary of missing values\n",
    "missing_summary = df.isnull().sum().reset_index()\n",
    "missing_summary.columns = ['Column', 'Missing Values']\n",
    "print(missing_summary)\n",
    "# Total number of missing values\n",
    "total_missing = df.isnull().sum().sum()\n",
    "print(f'Total missing values: {total_missing}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric columns\n",
    "summary_stats = df.describe()\n",
    "print(summary_stats)\n",
    "# Summary for non-numeric columns\n",
    "non_numeric_summary = df.describe(include=[object])\n",
    "print(non_numeric_summary)\n",
    "# Data types of each column\n",
    "data_types = df.dtypes\n",
    "print(data_types)\n",
    "# General information about the DataFrame\n",
    "info = df.info()\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed4bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Display the shape of the dataset\n",
    "rows, columns = df.shape\n",
    "print(f'The dataset has {rows} rows and {columns} columns.')\n",
    "\n",
    "# Summary statistics for numeric columns\n",
    "print(\"\\nSummary Statistics for Numeric Columns:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Summary for non-numeric columns\n",
    "print(\"\\nSummary for Non-Numeric Columns:\")\n",
    "print(df.describe(include=[object]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attribute:\n",
    "Definition: An attribute is a characteristic or property of an object. In programming, attributes hold data related to the object.\n",
    "Syntax: It is accessed directly without parentheses. For instance, df.shape is an attribute. It gives you the shape of the DataFrame df without needing to execute a function.\n",
    "Example: If df is a DataFrame object in pandas, df.shape returns a tuple representing the number of rows and columns in df.\n",
    "Method:\n",
    "Definition: A method is a function that is associated with an object and performs some action or computation using the object's data.\n",
    "Syntax: It is called with parentheses. For instance, df.describe() is a method. It performs a computation (in this case, generating descriptive statistics) and returns a result.\n",
    "Example: For the DataFrame df, df.describe() generates summary statistics such as mean, standard deviation, and percentiles for the numerical columns.\n",
    "Summary: \n",
    "Attributes represent static properties of an object and are accessed directly (like df.shape), while methods are functions that perform actions or computations on the object's data and are called with parentheses (like df.describe()). In essence, attributes give you information about the object, whereas methods execute operations and provide results based on the object's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06cad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "Count:\n",
    "Definition: The number of non-missing values in each column. It indicates how many entries were present for each variable.\n",
    "Mean:\n",
    "Definition: The average value of the entries in each column. It is calculated as the sum of all values divided by the count of values.\n",
    "Standard Deviation (std):\n",
    "Definition: A measure of the amount of variation or dispersion in the values of each column. A high standard deviation indicates that values are spread out over a wider range, while a low standard deviation indicates that values tend to be close to the mean.\n",
    "Minimum (min):\n",
    "Definition: The smallest value in each column. It provides the lowest point of the data distribution.\n",
    "    \n",
    "25th Percentile (25%):\n",
    "Definition: The value below which 25% of the data falls. It is also known as the first quartile (Q1). This gives a sense of the lower end of the data distribution.\n",
    "50th Percentile (50%):\n",
    "Definition: The median value of the data. It is the middle point of the data distribution, where 50% of the values are below and 50% are above. It divides the data into two equal halves.\n",
    "75th Percentile (75%):\n",
    "Definition: The value below which 75% of the data falls. It is also known as the third quartile (Q3). This provides insight into the upper end of the data distribution.\n",
    "Maximum (max):\n",
    "Definition: The largest value in each column. It provides the highest point of the data distribution.\n",
    "    \n",
    "Summary: These summary statistics provide a comprehensive view of the distribution and variability of data in each column, helping to understand the central tendency, spread, and range of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bab4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Handling Missing Data Across Rows (Using df.dropna()):\n",
    "df.dropna(): This function removes rows with missing data. If you use df.dropna(), it will delete any row where at least one value is missing. This can be useful when you want to keep only the complete rows of data. However, if many rows have missing values, this approach might lead to significant data loss.\n",
    "Handling Missing Data Down Columns (Using del df['col']):\n",
    "del df['col']: This command deletes an entire column from the DataFrame. If a column contains many missing values, you might choose to remove that column entirely if it’s not crucial for your analysis. This helps in cleaning up the dataset by getting rid of columns that might have too many gaps or are not useful for your analysis.\n",
    "\n",
    "Brief Answers:\n",
    "When to Use df.dropna(): Apply this function when you need to remove rows that contain missing values to ensure that your analysis is based only on complete cases. It is useful when missing data is scattered across multiple rows, and you prefer to work with rows that have all required values.\n",
    "When to Use del df['col']: Use this command when you have columns with missing data that are not valuable for your analysis or when a column has a large amount of missing data. This helps to clean up your DataFrame by removing unnecessary or problematic columns.\n",
    "Summary: df.dropna() is useful for eliminating rows with any missing values, while del df['col'] is effective for removing entire columns that contain missing data. Choosing the right method depends on whether you want to preserve rows or columns and the extent of missing data in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850204b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Understanding `df.groupby(\"col1\")[\"col2\"].describe()`\n",
    "\n",
    "1. **`df.groupby(\"col1\")`**:\n",
    "   - This groups the DataFrame `df` by the values in column `col1`. It essentially creates a new DataFrame where the rows are aggregated based on unique values of `col1`.\n",
    "\n",
    "2. **`[\"col2\"]`**:\n",
    "   - After grouping, this selects column `col2` from the grouped DataFrame. We are interested in performing operations on this specific column within each group.\n",
    "\n",
    "3. **`.describe()`**:\n",
    "   - This method provides summary statistics for `col2` within each group. For each unique value in `col1`, it calculates the count, mean, standard deviation, minimum, and percentiles (25%, 50%, 75%), and the maximum of `col2`.\n",
    "\n",
    "### Example with the Titanic Dataset\n",
    "\n",
    "Let's use a different example with the Titanic dataset. Assume we have the Titanic data loaded into a DataFrame `df_titanic` and we want to analyze the `fare` column based on the `pclass` (passenger class). \n",
    "\n",
    "Here’s a step-by-step guide on how you can do this:\n",
    "\n",
    "1. **Load the Titanic Dataset** (assuming you have it available):\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "\n",
    "   # Load Titanic dataset (replace the path with your dataset path)\n",
    "   df_titanic = pd.read_csv('path_to_titanic_dataset.csv')\n",
    "   ```\n",
    "\n",
    "2. **View Columns** to understand the structure:\n",
    "\n",
    "   ```python\n",
    "   print(df_titanic.columns)\n",
    "   ```\n",
    "\n",
    "   You should see something like:\n",
    "   ```\n",
    "   Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked'], dtype='object')\n",
    "   ```\n",
    "\n",
    "3. **Apply `groupby` and `describe`**:\n",
    "\n",
    "   ```python\n",
    "   # Group by 'Pclass' and describe 'Fare'\n",
    "   fare_description_by_pclass = df_titanic.groupby('Pclass')['Fare'].describe()\n",
    "   print(fare_description_by_pclass)\n",
    "   ```\n",
    "\n",
    "### Explanation of the Output\n",
    "\n",
    "Let's say the Titanic dataset output looks like this:\n",
    "\n",
    "```\n",
    "           count       mean         std   min     25%   50%     75%   max\n",
    "Pclass                                                                      \n",
    "1       216.0  84.154687  50.846050  26.00  30.00  62.00  104.00  512.33\n",
    "2       184.0  20.662183  14.642229  10.50  12.00  15.50   27.00  73.50\n",
    "3       491.0  13.675550  11.418020   0.00   7.91  10.50   18.00  69.55\n",
    "```\n",
    "\n",
    "- **`count`**: Number of passengers in each class for whom `Fare` data is available.\n",
    "- **`mean`**: Average fare paid by passengers in each class.\n",
    "- **`std`**: Standard deviation of the fare in each class, showing the variability.\n",
    "- **`min`**: Minimum fare paid in each class.\n",
    "- **`25%`**: The 25th percentile of fares in each class.\n",
    "- **`50%`**: The median (50th percentile) fare in each class.\n",
    "- **`75%`**: The 75th percentile of fares in each class.\n",
    "- **`max`**: Maximum fare paid in each class.\n",
    "\n",
    "**Summary**: `df.groupby(\"col1\")[\"col2\"].describe()` provides a set of descriptive statistics for a specified column, broken down by groups defined by another column. In our Titanic example, it gives a detailed view of fare distributions across different passenger classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. `df.describe()` and Its Count Values\n",
    "\n",
    "- **Purpose**: `df.describe()` provides summary statistics for each column in the DataFrame `df`, focusing on all the values present in the dataset.\n",
    "- **Count Values**: The `count` in `df.describe()` reflects the number of non-missing values for each column. This gives you an overview of the completeness of each column.\n",
    "\n",
    "**Fundamental Points**:\n",
    "- **Overall Missingness**: The `count` in `df.describe()` shows how many values are present for each column out of the total rows. Missing values in any column will reduce this count.\n",
    "- **Column-Specific Information**: This count is specific to each column and tells you how many non-missing entries there are, which helps identify if there are significant missing values in any column.\n",
    "\n",
    "### 2. `df.groupby(\"col1\")[\"col2\"].describe()` and Its Count Values\n",
    "\n",
    "- **Purpose**: `df.groupby(\"col1\")[\"col2\"].describe()` provides summary statistics for `col2` within each group defined by `col1`. This approach examines how the data in `col2` varies across different values of `col1`.\n",
    "- **Count Values**: The `count` in this context reflects the number of non-missing values of `col2` for each unique value of `col1`.\n",
    "\n",
    "**Fundamental Points**:\n",
    "- **Group-Specific Insights**: The `count` here provides insights into the availability of data for `col2` within each group defined by `col1`. It is crucial for understanding data distribution and completeness within each group.\n",
    "- **Impact of Grouping**: This count shows how many non-missing values of `col2` are present for each group, giving you an idea of how missingness varies across different categories or groups in `col1`.\n",
    "\n",
    "### Comparison\n",
    "\n",
    "- **`df.describe()` Count**:\n",
    "  - Reflects the completeness of each column in the overall dataset.\n",
    "  - Highlights missing data across the entire dataset without considering any group-based context.\n",
    "\n",
    "- **`df.groupby(\"col1\")[\"col2\"].describe()` Count**:\n",
    "  - Reflects the completeness of `col2` within each group defined by `col1`.\n",
    "  - Provides a breakdown of how much data is available within each group, which is essential for understanding group-specific patterns and insights.\n",
    "\n",
    "**Key Difference**:\n",
    "- **Overall Dataset vs. Group-Level Analysis**: `df.describe()` gives a broad view of missing values and completeness across the whole dataset, while `df.groupby(\"col1\")[\"col2\"].describe()` offers a more granular view, showing missingness and data distribution within specific groups. This group-level analysis can reveal important insights about how data availability differs across categories, which is crucial for understanding variations and ensuring the reliability of group-based analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f691a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
