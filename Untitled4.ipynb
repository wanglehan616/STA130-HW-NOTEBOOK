{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116dafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Key factor for testable ideas in hypothesis testing:\n",
    "\n",
    "The key factor that makes an idea testable statistically is **whether the idea can be quantified and measured**. A testable idea must have data that can be collected, and the results must be able to be subjected to statistical analysis. Ideas that are vague, subjective, or lacking in measurable outcomes cannot be examined statistically.\n",
    "\n",
    "### Criteria for a good null hypothesis:\n",
    "\n",
    "A good null hypothesis is **specific, measurable, and falsifiable**. It should state that there is **no effect or no difference** between groups or variables being studied. The null hypothesis acts as the default assumption that there is no relationship or change, and it must be clear enough to be tested statistically.\n",
    "\n",
    "### Difference between null hypothesis and alternative hypothesis:\n",
    "\n",
    "**Null Hypothesis (H₀):** The null hypothesis is the statement that there is **no effect, no difference, or no relationship** between variables. It serves as the default or starting assumption and is tested to see if it can be rejected based on evidence from the data.\n",
    "  \n",
    "**Alternative Hypothesis (H₁ or Hₐ):** The alternative hypothesis is the statement that there **is an effect, difference, or relationship** between variables. It is what the researcher aims to support or prove, and it is tested against the null hypothesis.\n",
    "\n",
    "In hypothesis testing, you gather data to determine whether there is enough evidence to reject the null hypothesis in favor of the alternative hypothesis. If the evidence is strong enough, the null hypothesis is rejected, suggesting that the alternative hypothesis may be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "To explain the sentence \"It is important to note that outcomes of tests refer to the population parameter, rather than the sample statistic!\" in simple terms for a non-statistical audience, let’s first define a few key concepts:\n",
    "\n",
    "1. **Population Parameter (μ):** This is the true average or value we are interested in for the entire group or population. For example, it could be the average height of all people in a country. It’s the actual number, but we usually don’t know this value.\n",
    "   \n",
    "2. **Sample (x₁, x₂, ..., xₙ):** A sample consists of individual values collected from the population. Since we can’t measure everyone in the population, we collect a sample (a smaller group) and use these values to estimate the population parameter.\n",
    "\n",
    "3. **Sample Statistic (x̄):** The sample statistic, like the average (x̄) of the sample values, is what we calculate from the data we collected. It helps us make an educated guess about the population parameter.\n",
    "\n",
    "4. **Null Hypothesis (H₀):** This is a starting assumption that there is no effect or no difference, meaning we assume the population parameter (μ) is a certain value (for example, H₀: μ = 50).\n",
    "\n",
    "What the sentence means:\n",
    "When we perform a hypothesis test, we are **not testing the sample statistic (x̄)** that we calculated from the sample. Instead, we are testing whether the **population parameter (μ)** — the true average for the entire population — matches the value we assumed under the null hypothesis. The outcome of the test helps us decide if the **population’s average (μ)** is likely to be different from what we initially assumed, based on the data in the sample.\n",
    "\n",
    "Summary:\n",
    "- We use sample data to calculate sample statistics (like the sample average, x̄).\n",
    "- We then use those statistics to make inferences about the population parameter (μ).\n",
    "- Hypothesis testing aims to determine whether the population parameter (μ) likely equals the value assumed in the null hypothesis, not just whether the sample statistic (x̄) matches.\n",
    "\n",
    "In simpler terms, when we test a hypothesis, we are trying to learn about the whole population, even though we are working with just a small part of it (the sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aca3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "When we calculate a p-value, we are essentially asking, \"How likely is it to get the results we observed (or more extreme) if the null hypothesis were true?\" \n",
    "\n",
    "To explain this more clearly:\n",
    "\n",
    "**Null Hypothesis (H₀):** This is our starting assumption. We assume that there is no effect, no difference, or no relationship in the population. For example, if we are testing whether a new drug works, the null hypothesis would say that the drug has no effect.\n",
    "\n",
    "**Imagine a world where the null hypothesis is true:** We imagine that the null hypothesis is actually correct — that the effect or difference we are testing for does not exist in the population. Then, we think about how the data would behave if this assumption were true.\n",
    "\n",
    "**Sampling distribution under the null hypothesis:** If the null hypothesis is true, we can use statistical theory to predict the distribution of possible outcomes we might get from different samples (called the **sampling distribution**). This gives us an idea of what kinds of sample results we would expect just due to random chance.\n",
    "\n",
    "**Relevance of the p-value:** The p-value tells us the probability of getting the observed sample result, or one more extreme, **if** the null hypothesis is true. If the p-value is very small, it means that the observed result is unlikely to happen just by chance if the null hypothesis is correct. This suggests that the null hypothesis might not be true, and we might consider rejecting it in favor of the alternative hypothesis.\n",
    "\n",
    "### Summary:\n",
    "We imagine a world where the null hypothesis is true when calculating the p-value because it helps us understand whether the observed data could reasonably happen due to random chance. The smaller the p-value, the less likely the data could occur if the null hypothesis were true, leading us to question the validity of the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ec336",
   "metadata": {},
   "outputs": [],
   "source": [
    "A smaller **p-value** makes the **null hypothesis (H₀)** look more \"ridiculous\" because it indicates that the data we observed is highly unlikely to occur if the null hypothesis were true.\n",
    "\n",
    "Here’s why:\n",
    "\n",
    "1. **Null Hypothesis (H₀):** This is our starting assumption, where we claim there is no effect or no difference. For example, we might assume two treatments have the same effect (H₀: the average effect of both treatments is the same).\n",
    "\n",
    "2. **Test Statistic:** This is a value calculated from the sample data that summarizes how far the data is from what we would expect if the null hypothesis were true. For example, it could be the difference in means between two groups.\n",
    "\n",
    "3. **Sampling Distribution Under the Null Hypothesis:** If the null hypothesis is true, we can create a distribution (the **sampling distribution**) of possible test statistics that could occur by random chance. This distribution shows the range of values we expect to see if the null hypothesis holds.\n",
    "\n",
    "4. **P-Value:** The p-value measures the probability of observing a test statistic as extreme (or more extreme) than what we got, **if the null hypothesis is true**. So, the smaller the p-value, the less likely it is that the observed data would happen by random chance if the null hypothesis were true.\n",
    "\n",
    "### Why the null hypothesis looks ridiculous with a small p-value:\n",
    "When the p-value is small, it means that our observed data falls in the extreme tails of the sampling distribution under the null hypothesis. This suggests that our observed results are very unusual or rare if the null hypothesis were correct. As a result, it becomes harder to believe that the null hypothesis is true, because getting such extreme data would be highly unlikely in that world.\n",
    "\n",
    "In simple terms: **a smaller p-value means the observed data is so unusual that sticking to the null hypothesis starts to seem unreasonable**. Therefore, we start doubting the null hypothesis and might consider rejecting it in favor of the alternative hypothesis, which could explain the data better.\n",
    "\n",
    "### Summary:\n",
    "A small p-value makes the null hypothesis look \"ridiculous\" because it shows that the data we observed is very unlikely to happen by chance if the null hypothesis were true, leading us to believe that the null hypothesis might be false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "To simulate a **p-value** using a \"50/50 coin-flipping\" model for the null hypothesis (H₀), we need to follow these steps:\n",
    "\n",
    "### Step 1: Understand the context\n",
    "- **Null hypothesis (H₀):** The population of humans does not have a preference for tilting their heads to the left or right when kissing. This implies that head tilting is a **50/50** random event, like flipping a fair coin. Under H₀, each kiss has an equal chance (50%) of resulting in a right tilt or a left tilt.\n",
    "  \n",
    "- **Observed data:** Out of 124 kissing couples, 80 tilted their heads to the right, which is **64.5%** of the total. This is higher than the 50% expected under H₀.\n",
    "\n",
    "### Step 2: Simulate a \"50/50 world\" where H₀ is true\n",
    "- We will simulate flipping a fair coin for 124 couples, where each coin flip represents one couple, with \"heads\" indicating a right tilt and \"tails\" indicating a left tilt.\n",
    "- We can repeat this simulation many times (e.g., 10,000 times) to see how often we observe 80 or more right tilts (or a percentage ≥ 64.5%).\n",
    "\n",
    "### Step 3: Compute the p-value\n",
    "- The **p-value** is the probability of observing a result as extreme as (or more extreme than) 80 right tilts out of 124 couples, assuming H₀ is true.\n",
    "- In each simulation, we count how often we get 80 or more right tilts, and then divide that count by the total number of simulations to get the p-value.\n",
    "\n",
    "### Step 4: Evaluate the strength of evidence against H₀\n",
    "Based on the p-value, we can interpret the strength of evidence against H₀ using the provided table:\n",
    "\n",
    "| p-value                | Evidence                                 |\n",
    "|------------------------|------------------------------------------|\n",
    "| **p > 0.1**            | No evidence against the null hypothesis  |\n",
    "| **0.05 < p ≤ 0.1**     | Weak evidence against the null hypothesis |\n",
    "| **0.01 < p ≤ 0.05**    | Moderate evidence against the null hypothesis |\n",
    "| **0.001 < p ≤ 0.01**   | Strong evidence against the null hypothesis |\n",
    "| **p ≤ 0.001**          | Very strong evidence against the null hypothesis |\n",
    "\n",
    "### Example Simulation\n",
    "For example, if after 10,000 simulations, we observe that 500 simulations resulted in 80 or more right tilts, the p-value would be:\n",
    "\n",
    "\\[\n",
    "\\text{p-value} = \\frac{500}{10,000} = 0.05\n",
    "\\]\n",
    "\n",
    "This p-value would suggest **moderate evidence** against the null hypothesis (based on the table), indicating that the observed result of 80 right tilts is somewhat unusual if head tilting were truly random.\n",
    "\n",
    "### Conclusion\n",
    "If the p-value is small (e.g., 0.01 or lower), this suggests **strong evidence** against the null hypothesis, meaning that head tilting might not be random, and people might have a preference for tilting to the right. Conversely, if the p-value is large (e.g., above 0.1), we have little to no evidence to reject the null hypothesis, and the observed right tilting could just be due to random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399c96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "No, a **smaller p-value** does not definitively prove that the **null hypothesis (H₀)** is false. In hypothesis testing, a p-value helps us decide whether the data provides strong enough evidence to **reject** H₀, but it doesn’t give a definitive proof.\n",
    "\n",
    "### 1. **Can a p-value definitively prove the null hypothesis is false?**\n",
    "No, a small p-value indicates that the observed data is unlikely under the null hypothesis, but it doesn’t **prove** H₀ is false. We can only say that the data provides **strong evidence** against H₀, not definitive proof.\n",
    "\n",
    "### 2. **Can a p-value definitively prove Fido is innocent?**\n",
    "No, a p-value can’t definitively prove **innocence**. If the p-value is large (high), it means there is not enough evidence to reject the null hypothesis (in this case, assuming Fido is innocent). However, that doesn’t **prove** innocence—it only suggests that the data is consistent with Fido being innocent.\n",
    "\n",
    "### 3. **Can a p-value definitively prove Fido is guilty?**\n",
    "No, a small p-value can’t definitively prove **guilt**. A small p-value suggests that the evidence strongly contradicts the null hypothesis (Fido being innocent), but it doesn't definitively prove guilt. There’s always a small chance that extreme data could happen by random chance, even if Fido is actually innocent.\n",
    "\n",
    "### 4. **How low or high does a p-value have to be to definitively prove one or the other?**\n",
    "A p-value, no matter how low or high, can never **definitively prove** guilt, innocence, or the null hypothesis. Hypothesis testing is about **likelihood and evidence**, not absolute proof. We use p-values to assess the strength of evidence, but there’s always some uncertainty involved.\n",
    "\n",
    "### Conclusion:\n",
    "A p-value helps us make decisions based on evidence but does not provide definitive proof. Even with very small or large p-values, there’s always some uncertainty, so we can never conclusively prove or disprove a hypothesis based solely on the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "To adjust the code from **Demo II of the Week 5 Tutorial** and compute a **one-sided** (or **one-tailed**) p-value, we need to modify the way we handle the p-value calculation, focusing on only one side (or tail) of the distribution, as opposed to both sides (as in the two-sided test).\n",
    "\n",
    "### Key Changes in the Code\n",
    "\n",
    "1. **Null Hypothesis and Alternative Hypothesis:**\n",
    "   - In the **two-sided test**, the null hypothesis (H₀) might be something like: \n",
    "     - H₀: There is **no difference** between two groups (e.g., the vaccine and placebo groups), and the alternative hypothesis (H₁) is that there **is a difference** in either direction (greater or lesser).\n",
    "   - In the **one-sided test**, the null hypothesis could be:\n",
    "     - H₀: There is no difference or the vaccine group's success rate is **less than or equal to** the placebo group’s.\n",
    "     - H₁: The vaccine group's success rate is **greater than** the placebo group’s. (For a one-tailed test focusing on one direction of improvement).\n",
    "\n",
    "2. **Modify the p-value Calculation:**\n",
    "   - In a **two-sided test**, we compute the p-value by considering the extreme results in **both** tails of the sampling distribution (e.g., values that are either much greater or much less than expected under H₀).\n",
    "   - In a **one-sided test**, we only focus on **one tail** of the distribution. Specifically, we calculate the probability of observing a result **at least as extreme** as the observed data in **one direction** (e.g., greater than or less than, depending on the hypothesis).\n",
    "\n",
    "   In Python (using simulation or a statistical package like `scipy`), you would:\n",
    "   - For the **two-sided test**, compute the p-value as:\n",
    "     ```python\n",
    "     p_value = 2 * min(left_tail_prob, right_tail_prob)\n",
    "     ```\n",
    "   - For the **one-sided test**, compute the p-value as:\n",
    "     ```python\n",
    "     p_value = right_tail_prob  # if testing for greater than\n",
    "     # or\n",
    "     p_value = left_tail_prob   # if testing for less than\n",
    "     ```\n",
    "\n",
    "3. **Interpreting the Change:**\n",
    "   - In the **one-tailed test**, you're only testing whether one group (e.g., the vaccine group) performs **better** (or **worse**, depending on the direction of the test) than the other.\n",
    "   - The **p-value** in a one-tailed test is generally **smaller** than in a two-tailed test because you're only considering evidence in one direction. This makes it easier to reject the null hypothesis if the data supports the alternative hypothesis in that specific direction.\n",
    "\n",
    "### Example of Change in Interpretation:\n",
    "- In a **two-sided test**, the question is: \"Is there a difference between the vaccine and placebo groups, whether positive or negative?\"\n",
    "- In a **one-sided test**, the question becomes more specific: \"Does the vaccine group perform **better** than the placebo group?\"\n",
    "\n",
    "### Expected p-value Difference:\n",
    "- In a **one-tailed test**, the p-value will typically be **smaller** because we are only considering one side of the distribution, meaning we are testing a more specific hypothesis. Therefore, we expect to find stronger evidence in favor of the alternative hypothesis if the data supports it.\n",
    "\n",
    "### Conclusion:\n",
    "The main difference is that a **one-tailed test** is more focused on detecting an effect in **one specific direction**. As a result, the **p-value will be smaller** compared to the two-tailed test, which looks at both directions of the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fisher's Tea Experiment – STA130 Student Tea Test Report\n",
    "\n",
    "### 1. **Problem Introduction**\n",
    "In this experiment, we want to investigate if 49 out of 80 STA130 students' correct identification of whether milk or tea was poured first is purely by chance, or if there's evidence that they are able to taste the difference. This experiment parallels Fisher’s famous tea-tasting experiment with Dr. Muriel Bristol, where she claimed she could tell the difference between tea poured first and milk poured first.\n",
    "\n",
    "The goal is to test the hypothesis that students in STA130 are simply guessing (null hypothesis) versus the alternative that they can actually tell the difference (alternative hypothesis).\n",
    "\n",
    "### 2. **Relationship to Fisher and Bristol's Original Experiment**\n",
    "In Fisher's experiment, Dr. Bristol correctly identified all 8 cups, raising the question of whether her success was purely by chance. Similarly, in this experiment, 49 out of 80 students correctly identified the order of pouring. The two experiments differ in sample size, population (students vs. a tea expert), and the level of expertise.\n",
    "\n",
    "### 3. **Hypotheses**\n",
    "\n",
    "- **Null Hypothesis (\\(H_0\\))**: The STA130 students are guessing randomly, with a 50% chance of correctly identifying whether tea or milk was poured first.\n",
    "  - Formal: \\( H_0: p = 0.5 \\), where \\( p \\) is the proportion of correct identifications (assuming random guessing).\n",
    "  - Informal: Students are just guessing and have no real ability to tell the difference between the order of pouring.\n",
    "\n",
    "- **Alternative Hypothesis (\\(H_1\\))**: The STA130 students have an ability to correctly identify the order of pouring better than random guessing.\n",
    "  - Formal: \\( H_1: p > 0.5 \\).\n",
    "  - Informal: Students can taste a difference between tea and milk when poured in a particular order, leading to a higher proportion of correct guesses than 50%.\n",
    "\n",
    "### 4. **Quantitative Analysis**\n",
    "\n",
    "To test the null hypothesis, we will simulate the experiment under the assumption that students are guessing randomly, which means each student has a 50% chance of guessing correctly. We will calculate the **p-value**, which represents the probability of obtaining a result as extreme or more extreme than 49 correct out of 80, assuming \\(H_0\\) is true.\n",
    "\n",
    "#### **Steps:**\n",
    "1. Simulate 100,000 trials where 80 students each have a 50% chance of guessing correctly.\n",
    "2. Count how many times we observe 49 or more correct answers.\n",
    "3. Estimate the p-value by calculating the proportion of simulations that resulted in 49 or more correct guesses.\n",
    "\n",
    "#### **Methodology Code:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_students = 80  # number of students\n",
    "n_correct = 49   # observed number of correct guesses\n",
    "p = 0.5          # probability of guessing correctly under H_0\n",
    "n_simulations = 100000  # number of simulations\n",
    "\n",
    "# Simulate the number of correct guesses for each trial\n",
    "simulated_data = np.random.binomial(n_students, p, size=n_simulations)\n",
    "\n",
    "# Calculate the proportion of simulations where the number of correct guesses >= observed correct guesses\n",
    "p_value = np.mean(simulated_data >= n_correct)\n",
    "\n",
    "print(f\"Estimated p-value: {p_value:.5f}\")\n",
    "```\n",
    "\n",
    "### 5. **Supporting Visualizations (optional)**\n",
    "\n",
    "We could visualize the distribution of correct guesses under the null hypothesis using a histogram. This would show the typical range of results if students were guessing randomly, and we would highlight where 49 correct guesses fall in the distribution.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram of simulated correct guesses\n",
    "plt.hist(simulated_data, bins=range(min(simulated_data), max(simulated_data)+1), edgecolor='black', alpha=0.7)\n",
    "plt.axvline(n_correct, color='red', linestyle='--', label=f'Observed = {n_correct}')\n",
    "plt.xlabel('Number of Correct Guesses')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Correct Guesses under Null Hypothesis')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 6. **Findings and Discussion**\n",
    "\n",
    "- **P-value**: The p-value calculated through simulation tells us the probability of observing 49 or more correct identifications if the null hypothesis of random guessing (50% chance) were true.\n",
    "- If the p-value is very low (e.g., below 0.05), we would have **strong evidence** against the null hypothesis, suggesting that the STA130 students can indeed tell the difference.\n",
    "- If the p-value is not small, it would mean that the observed result could easily happen by chance, and we would fail to reject the null hypothesis.\n",
    "\n",
    "### 7. **Conclusion regarding the Null Hypothesis**\n",
    "\n",
    "Based on the p-value, we can make a decision:\n",
    "\n",
    "- **If the p-value is small (p < 0.05)**: We would reject the null hypothesis, concluding that there is evidence that STA130 students can distinguish the order of pouring tea and milk better than random guessing.\n",
    "- **If the p-value is large (p ≥ 0.05)**: We would fail to reject the null hypothesis, indicating that the data does not provide strong evidence that students can tell the difference, and the results may be due to chance.\n",
    "\n",
    "### 8. **Confidence Interval Approach (optional)**\n",
    "\n",
    "Instead of (or in addition to) calculating a p-value, we could construct a **confidence interval** for the proportion of students who can correctly identify the order of pouring. A confidence interval would give us a range of plausible values for this proportion and help us assess whether 50% (random guessing) is a plausible value based on the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba582b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
