{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d35fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation of the Code and Results\n",
    "The code provided is an implementation of linear regression to assess the performance of different models, focusing on the \"in-sample\" and \"out-of-sample\" R-squared values to evaluate how well the model generalizes. Here's a breakdown of each section of the code and the concepts it illustrates:\n",
    "\n",
    "# 1.Dataset Splitting: \n",
    "```python\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train, pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train\n",
    "```\n",
    "-Purpose: The dataset `pokeaman` is split into two halves—**training** (`pokeaman_train`) and **testing** (`pokeaman_test`) data. This split is done using the `train_test_split` function from `sklearn.model_selection`. \n",
    "-Imputation: The code replaces any `NaN` (missing) values in the column \"Type 2\" with `'None'` to ensure no missing data remains.\n",
    "-Seed for Reproducibility: `np.random.seed(130)` ensures that the train-test split is reproducible every time the code runs.\n",
    "  \n",
    "# 2.Model Specification and Fitting (Simple Model):\n",
    "```python\n",
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense', \n",
    "                      data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()\n",
    "```\n",
    "-Model: This code defines and fits a simple linear regression model using the `statsmodels` library (`smf.ols`), where `HP` (hit points) is the dependent variable, and `Attack` and `Defense` are the independent variables. This is a basic model that assumes a linear relationship between the variables.\n",
    "-Fitting the Model: The `fit()` function estimates the coefficients of the model, and `summary()` generates the statistical summary (e.g., R-squared, p-values, coefficients).\n",
    "\n",
    "# 3.Making Predictions and Calculating In-Sample and Out-of-Sample R-squared:\n",
    "```python\n",
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y, yhat_model3)[0, 1]**2)\n",
    "```\n",
    "-Predictions: The `predict()` function is used to make predictions on the test data (`pokeaman_test`) based on the fitted model.\n",
    "-In-Sample R-squared: The `model3_fit.rsquared` value is the **R-squared** of the model on the training data (in-sample). It measures how much of the variance in the dependent variable (`HP`) is explained by the model using the training data.\n",
    "-Out-of-Sample R-squared: The `np.corrcoef(y, yhat_model3)[0, 1]**2` computes the **R-squared** for the test data, where `y` is the true values of `HP` and `yhat_model3` is the predicted values. This gives an indication of how well the model generalizes to new, unseen data.\n",
    "\n",
    "# 4.Model Specification and Fitting (Complex Model):\n",
    "```python\n",
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()\n",
    "```\n",
    "-Complex Model: A more complex regression model is specified, where `HP` is predicted by several interaction terms between variables (`Attack`, `Defense`, `Speed`, `Legendary`, etc.). This model includes higher-order interactions (products of variables), making it more complex than the first model.\n",
    "-Fitting the Model: The model is fitted to the training data (`pokeaman_train`) using the same process as the simpler model.\n",
    "\n",
    "# 5.Making Predictions and Calculating In-Sample and Out-of-Sample R-squared (Complex Model):\n",
    "```python\n",
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y, yhat_model4)[0, 1]**2)\n",
    "```\n",
    "-Predictions and R-squared: The process of making predictions (`yhat_model4`) and calculating both the in-sample and out-of-sample R-squared values is repeated for the more complex model. This allows for comparison of model performance.\n",
    "\n",
    "# Key Concepts and Insights\n",
    "-In-Sample R-squared: This measures the goodness of fit for the model on the data it was trained on. A higher in-sample R-squared indicates that the model explains a higher proportion of the variance in the training data.\n",
    "-Out-of-Sample R-squared: This measures how well the model performs on data it has not seen before (test data). It’s a critical measure of **generalizability** or how well the model will likely perform on new, unseen data.\n",
    "-Model Overfitting: If the **in-sample R-squared** is much higher than the **out-of-sample R-squared**, the model may be **overfitting** to the training data. Overfitting occurs when the model learns patterns specific to the training data, but fails to generalize well to new data.\n",
    "- Comparison Between Models: The code compares a simple model (`Attack + Defense`) and a complex model with interactions. The performance metrics (R-squared values) allow us to assess whether the more complex model provides substantial improvements in predicting out-of-sample data, or if it suffers from overfitting.\n",
    "\n",
    "# Conclusion\n",
    "The code is illustrating the concept of model evaluation by comparing in-sample and out-of-sample R-squared values for two models: a simple one and a complex one. This exercise helps assess whether the models are generalizing well to new data or if they are overfitting the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ce494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation of the Code and Concepts\n",
    "The code provided explores **model complexity** and **multicollinearity** in the context of linear regression. Specifically, it highlights how overly complex models (with many predictor variables and interactions) may suffer from poor **out-of-sample generalization**, often due to issues like multicollinearity. Here’s a breakdown of key points:\n",
    "\n",
    "# 1.Design Matrix and Predictor Variables\n",
    "- In linear regression, the \"design matrix\" (referred to as `model_spec.exog`) is the matrix of predictor variables that is used to fit the model. Each column represents a predictor (or a transformation of a predictor, such as an interaction term). \n",
    "- The linear form of `model4` introduces many predictor variables, some of which are **interaction terms** (e.g., `Attack * Defense * Speed * Legendary`). These interactions multiply different variables together to form new predictors.  \n",
    "- As more interaction terms are included in the model, the design matrix becomes more complex. This is what happens in `model4_linear_form`, where multiple variables (including interaction terms) are included. As a result, the number of predictors increases significantly.\n",
    "\n",
    "# 2.Multicollinearity and Generalization\n",
    "- **Multicollinearity** occurs when two or more predictor variables in the model are highly correlated. This leads to redundancy in the predictors, making it harder to estimate the model’s coefficients accurately. In simple terms, it means that the model is having difficulty distinguishing between the effects of different predictors because they are too similar.  \n",
    "- The correlation matrix of the **design matrix** (`np.corrcoef(model4_spec.exog)`) shows the pairwise correlations between the predictors. If there are high correlations (close to 1 or -1), this indicates multicollinearity, meaning some predictors are highly correlated with others.\n",
    "- Multicollinearity can cause problems in **out-of-sample generalization**. This is because a model that fits perfectly to the training data (in-sample) may be overly sensitive to the training data's specific noise or idiosyncrasies. When the model is tested on unseen data, it might not perform well because it has essentially \"overfitted\" the training data by modeling patterns that are not truly generalizable.\n",
    "\n",
    "# 3.Condition Number as a Diagnostic\n",
    "- The condition number is a diagnostic measure that indicates the extent of multicollinearity in the design matrix. A very **high condition number** suggests significant multicollinearity, meaning the model’s predictors are highly correlated, which can lead to unstable estimates and poor generalization.  \n",
    "- In the code:\n",
    "  - `model3_fit.summary().tables[-1]` shows a condition number of 343 without centering and scaling, which indicates a potential problem with multicollinearity.\n",
    "  - After centering and scaling the predictors in `model3`, the condition number drops significantly to **1.66**, suggesting much lower multicollinearity.\n",
    "  - On the other hand, in `model4`, even after centering and scaling, the condition number remains **very large** (around **2.25 trillion**), indicating severe multicollinearity.\n",
    "\n",
    "# 4.Overfitting and Model Complexity\n",
    "- **Overfitting** happens when the model is too complex relative to the amount of data available. In the case of `model4`, the large number of interaction terms (e.g., `Attack * Defense * Speed * Legendary`) results in a model that fits the training data very well but is not generalizable to the test data. This is because the model has likely learned patterns that are specific to the training data, including noise that won’t be present in future data.\n",
    "- By contrast, `model3` is simpler (only using `Attack` and `Defense`) and has a lower R-squared, but its simpler form allows it to generalize better to the testing data.\n",
    "\n",
    "# 5.Centering and Scaling of Predictors\n",
    "- **Centering** refers to subtracting the mean of each predictor variable, and **scaling** refers to dividing each predictor by its standard deviation. This helps to standardize the predictors, making them comparable in magnitude and reducing the effects of large differences in scale between variables.  \n",
    "- Centering and scaling also **reduce the condition number** of the design matrix, helping to alleviate multicollinearity, which is why it’s considered good practice when dealing with multiple linear regression models with continuous predictors.\n",
    "\n",
    "# Key Takeaways\n",
    "- **Multicollinearity** in `model4` makes it difficult for the model to generalize to out-of-sample data because the predictors are highly correlated, leading to unreliable estimates. This causes **overfitting**: the model fits well to the training data but performs poorly on new, unseen data.\n",
    "- The **condition number** is a valuable diagnostic tool that indicates the severity of multicollinearity. A high condition number suggests multicollinearity, making the model's generalizability more uncertain.\n",
    "- **Simpler models**, like `model3`, are less likely to suffer from overfitting because they rely on fewer, less correlated predictors, and can generalize better to new data.\n",
    "- **Centering and scaling** continuous predictors help reduce multicollinearity and make the condition number more reliable, highlighting whether the model is suffering from multicollinearity and helping identify models that are more stable and generalizable.\n",
    "\n",
    "In summary, the excessive complexity in `model4` introduces multicollinearity, which causes overfitting and poor generalization. Centering and scaling predictors can help mitigate these issues by reducing the condition number and providing more stable estimates for model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb70d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extension and Development of Models: A Concise Explanation\n",
    "\n",
    "#Model 3 to Model 5:\n",
    "-Model 3 was a simpler model focused on key attributes like `Attack` and `Defense`.\n",
    "-Model 5 extends Model 3 by adding more predictors. This includes continuous variables (`Speed`, `Sp. Def`, `Sp. Atk`) and categorical variables such as `Generation` and `Type 1`/`Type 2` using indicator (dummy) variables. This addition of more predictors helps capture a broader range of effects, potentially improving the model’s performance.\n",
    "\n",
    "#Model 5 to Model 6:\n",
    "-Model 6 refines Model 5 by simplifying the predictors. Here, some predictors from Model 5 are removed (e.g., `Defense`), and more focused categorical indicators are added (`Type 1 == Normal`, `Generation == 2 and 5`). This version keeps only significant predictors and tries to maintain the balance between model complexity and generalizability.\n",
    "\n",
    "#Model 6 to Model 7:\n",
    "- Model 7 further extends Model 6 by introducing interaction terms between predictors, like `Attack  Speed  Sp. Def * Sp. Atk`. This adds complexity, capturing potential joint effects between variables. These interactions could reveal hidden relationships that are missed in simpler models, although they also increase the risk of overfitting if not handled carefully.\n",
    "\n",
    "#Centering and Scaling for Model 7:\n",
    "- Model 7 is then centered and scaled, meaning continuous predictors are standardized to have a mean of zero and a standard deviation of one. This helps reduce multicollinearity and ensures that coefficients are comparable across predictors. The condition number drops significantly (from 2.34 trillion to 15.4), indicating that multicollinearity is not a major issue.\n",
    "\n",
    "#Summary in Simple Terms:\n",
    "1.Model 5 is an expansion of **Model 3**, adding more variables to capture additional relationships.\n",
    "2.Model 6 refines **Model 5** by removing less useful predictors and focusing on significant categorical indicators.\n",
    "3.Model 7 adds complexity again with interactions, capturing how predictors influence each other in combination, potentially improving accuracy.\n",
    "4.Centering and scaling** in **Model 7** helps reduce multicollinearity and improve the stability of the model, resulting in a more reliable prediction.\n",
    "\n",
    "In general, this process involves gradually improving the model by adding variables, refining them based on significance, and addressing potential issues like multicollinearity to ensure better prediction performance both in-sample and out-of-sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e02980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation of the Demonstration\n",
    "This task involves running repeated training and testing of a model to observe the variation in its performance both on the training set (\"in-sample\") and the test set (\"out-of-sample\"). By iterating over the data split multiple times, we can assess how well the model generalizes across different subsets of the data. The main goal here is to demonstrate how **overfitting** and **underfitting** manifest in real-world data splits.\n",
    "\n",
    "Here’s the core logic:\n",
    "1. **Overfitting**: This occurs when a model performs well on the training set but poorly on the test set. It suggests that the model is too complex and is capturing noise or irrelevant details from the training data, which does not generalize well.\n",
    "2. **Underfitting**: If the model performs poorly on both the training and test sets, it might not be capturing enough complexity in the data to make accurate predictions.\n",
    "\n",
    "In this demonstration:\n",
    "- We use the **R-squared** metric for both in-sample and out-of-sample performance. \n",
    "- **In-sample R-squared** measures how well the model fits the training data.\n",
    "- **Out-of-sample R-squared** measures how well the model generalizes to unseen data (the test set).\n",
    "\n",
    "By running the model multiple times with different random splits, we can track how these R-squared values fluctuate, revealing the model’s performance consistency and indicating the presence of overfitting or underfitting.\n",
    "\n",
    "# Code Implementation (for demonstration):\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Assuming `songs` is your DataFrame\n",
    "linear_form = 'danceability ~ energy * loudness + energy * mode'\n",
    "\n",
    "#Define the number of repetitions\n",
    "reps = 100\n",
    "in_sample_Rsquared = np.array([0.0]*reps)\n",
    "out_of_sample_Rsquared = np.array([0.0]*reps)\n",
    "\n",
    "for i in range(reps):\n",
    "    # Split the data\n",
    "    songs_training_data, songs_testing_data = train_test_split(songs, train_size=31)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    final_model_fit = smf.ols(formula=linear_form, data=songs_training_data).fit()\n",
    "    \n",
    "    # Store the in-sample R-squared\n",
    "    in_sample_Rsquared[i] = final_model_fit.rsquared\n",
    "    \n",
    "    # Calculate the out-of-sample R-squared\n",
    "    out_of_sample_Rsquared[i] = np.corrcoef(songs_testing_data.danceability, \n",
    "                                            final_model_fit.predict(songs_testing_data))[0, 1]**2\n",
    "\n",
    "#Create a DataFrame to hold the results\n",
    "df = pd.DataFrame({\n",
    "    \"In Sample Performance (Rsquared)\": in_sample_Rsquared,\n",
    "    \"Out of Sample Performance (Rsquared)\": out_of_sample_Rsquared})\n",
    "\n",
    "#Create a scatter plot\n",
    "fig = px.scatter(df, x=\"In Sample Performance (Rsquared)\", \n",
    "                 y=\"Out of Sample Performance (Rsquared)\", \n",
    "                 title=\"In-Sample vs Out-of-Sample Model Performance\")\n",
    "\n",
    "#Add the line y=x (perfect correlation) to the plot for reference\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], name=\"y=x\", line_shape='linear'))\n",
    "\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "# Explanation of the Code:\n",
    "1.Loop through Repetitions (`reps`): The loop runs 100 times, splitting the dataset into a training and testing set in each iteration.\n",
    "2.Model Fitting: Within each iteration, the model is fitted to the training set using **Ordinary Least Squares** (OLS), and R-squared values for both the training (in-sample) and testing (out-of-sample) datasets are calculated.\n",
    "3.Out-of-sample Prediction: The out-of-sample R-squared is computed by comparing the model’s predictions on the test set with the actual values in the test set.\n",
    "4.Visualization: A scatter plot is created to visualize the relationship between in-sample and out-of-sample performance. A line representing **y=x** is also added to indicate where in-sample performance would match out-of-sample performance if the model perfectly generalizes.\n",
    "\n",
    "### Interpretation of the Results:\n",
    "1.In-Sample vs Out-of-Sample Comparison**: \n",
    "   - If the in-sample R-squared is much higher than the out-of-sample R-squared, this indicates that the model is **overfitting**. The model is too complex and has \"learned\" patterns specific to the training data that do not generalize well to unseen data.\n",
    "   - If the in-sample and out-of-sample R-squared values are close to each other, the model is likely **generalizing well**, meaning it performs similarly on both the training and test data.\n",
    "   - If both values are low, it might suggest **underfitting**, where the model is too simple and fails to capture the underlying patterns in the data.\n",
    "\n",
    "2.Purpose of Demonstration**:\n",
    "   - This demonstration provides insight into how models behave across different data splits and helps diagnose overfitting or underfitting issues by comparing performance metrics.\n",
    "   - By running the model over multiple random splits and visualizing the results, you can get a better understanding of how robust and reliable your model is, and whether adjustments (like simplifying or adding complexity to the model) are necessary to improve generalization. \n",
    "\n",
    "In summary, the demonstration shows that by repeatedly splitting the data and comparing in-sample and out-of-sample performance, we can evaluate how well the model generalizes to new data, and identify potential issues like overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cad17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "This illustration highlights a critical concept in model selection and generalization, where a more complex model with higher in-sample performance (like `model7_fit`) may not generalize as well to new data compared to a simpler, more interpretable model (like `model6_fit`). Here’s a breakdown of what’s going on:\n",
    "\n",
    "# 1.Complexity vs. Performance Trade-off\n",
    "   - **Model Complexity**: `model7_fit` includes many interaction terms, creating a very detailed fit for the training data. While this detail can improve in-sample performance, it also makes the model more susceptible to overfitting. Overfitting happens when a model captures specific patterns or noise unique to the training data, which may not generalize to new data.\n",
    "   - **Model6_fit Simplicity**: `model6_fit` is a simpler model with fewer terms. While it might not capture every nuance in the training data, it generally fits the broader, essential patterns, making it potentially more reliable for predicting unseen data.\n",
    "\n",
    "# 2.Importance of Interpretability\n",
    "   - **Interpretability Concerns**: Complex models with many interactions, such as `model7_fit`, are harder to interpret, particularly with high-order interactions that lack clear, intuitive explanations. This complexity can create practical challenges, especially when clear, actionable insights from the model are needed.\n",
    "   - **Parsimonious Models**: In fields where interpretability matters as much as predictive power (e.g., medical, financial, and policy-related applications), a more parsimonious model, like `model6_fit`, is often preferred even if it sacrifices some degree of raw accuracy. This is because its simpler structure makes it easier to understand, communicate, and validate.\n",
    "\n",
    "# 3.Evaluating Generalizability on Future Data\n",
    "   - **Sequential Prediction Simulation**: This part simulates how the models would perform if we were using data from earlier generations of Pokémon to predict stats for future generations. The demonstration reveals that while both models encounter generalizability challenges when using data from previous generations, `model7_fit` struggles more.\n",
    "   - **Future Data Performance**: By isolating Generations 1–5 and predicting Generation 6 stats, the test reveals that `model7_fit`, with its complex specifications, shows weaker performance when predicting on genuinely new data. This provides evidence that simpler models, even if they show slightly lower in-sample performance, may still outperform complex models on unseen data.\n",
    "\n",
    "# 4.Key Takeaways\n",
    "   - **Caution with Complexity**: Just because a model has higher in-sample performance does not mean it will generalize better. Complexity should be added to a model only if there is strong evidence it truly enhances predictive ability across new datasets.\n",
    "   - **Preference for Simplicity in Comparable Models**: When two models have similar out-of-sample performance, it’s often better to choose the simpler model due to its interpretability and potentially more consistent generalizability.\n",
    "   - **Practical Data Flow Simulation**: Evaluating models with data that mimics real-world conditions, such as sequentially arriving data, provides a more realistic assessment of generalizability than purely random train-test splits.\n",
    "\n",
    "In essence, this illustration reminds us that model evaluation should consider not only raw performance metrics but also interpretability, generalizability, and real-world application settings, where simpler, more interpretable models often have an advantage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
